{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B01_SJ_news_crawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1yoDaO6MFX5N0p-3P2uUrOEJ92NGzrgtP",
      "authorship_tag": "ABX9TyMlNiiTw5dhHzWeW0bU51ev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyoungmiKwon/Bigdata_Training_at_ITwill/blob/main/Team_Project/B01_SJ_news_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9568XGB5SWt"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "#저장 경로 지정\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/삼성전자')\n",
        "\n",
        "#===============================================================================================================================\n",
        "# 코드 참조, company_list 다운\n",
        "# https://blog.naver.com/hengju2/222132075354\n",
        "# 종목번호 {'삼성전자':'005930', 'LG전자':'066570', 'JYP Ent.':'035900', '와이지엔터테인먼트':'122870'}\n",
        "#===============================================================================================================================\n",
        "\n",
        "def crawler(company,company_code,startpage, maxpage):\n",
        "    result_x={\"date\" : [], \"source\" :[], \"title\" : [],\"url\" :[], 'article': []}\n",
        "    data_x=pd.DataFrame(result_x)\n",
        "\n",
        "    page = int(startpage)\n",
        "\n",
        "    while page <= int(maxpage):\n",
        "        url = 'https://finance.naver.com/item/news_news.nhn?code=' + str(company_code) + '&page=' + str(page)\n",
        "        source_code = requests.get(url).text\n",
        "        html = BeautifulSoup(source_code, \"lxml\")\n",
        "\n",
        "        # 뉴스 제목\n",
        "        titles = html.select('.title')\n",
        "\n",
        "        title_result=[]\n",
        "        for title in titles:\n",
        "            title = title.get_text()\n",
        "            title = re.sub('\\n','',title)\n",
        "            title_result.append(title)\n",
        "\n",
        "        # 뉴스 링크, 기사\n",
        "        links = html.select('.title')\n",
        "\n",
        "        link_result =[]\n",
        "        art_mainn = []\n",
        "        try:\n",
        "            for link in links:\n",
        "                add = 'https://finance.naver.com' + link.find('a')['href']\n",
        "                response = requests.get(add).text\n",
        "                htmlx = BeautifulSoup(response, \"lxml\")\n",
        "                selector='#news_read'\n",
        "                art_main1 = htmlx.select(selector)\n",
        "                for tag in art_main1[0].find_all('a'):  \n",
        "                    tag.string.replace_with('')\n",
        "                art_main = art_main1[0].text.replace('관련뉴스해당 언론사에서 선정하며 언론사 페이지(아웃링크)로 이동해 볼 수 있습니다.','').strip()\n",
        "                art_mainn.append(art_main)\n",
        "\n",
        "                link_result.append(add)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "        # 뉴스 날짜\n",
        "        dates = html.select('.date')\n",
        "        date_result = [date.get_text() for date in dates]\n",
        "\n",
        "        # 뉴스 매체\n",
        "        sources = html.select('.info')\n",
        "        source_result = [source.get_text() for source in sources]\n",
        "\n",
        "        # 다운로드\n",
        "        \n",
        "        result= {\"date\" : date_result, \"source\" : source_result, \"title\" : title_result,\"url\" : link_result, 'article': art_mainn}\n",
        "        df_result = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in result.items()])) \n",
        "        data_x=pd.concat([data_x,df_result],ignore_index=True)\n",
        "        print(\"다운 받고 있습니다------\")\n",
        "        page += 1\n",
        "\n",
        "    data_x.drop_duplicates([\"title\"],keep = 'last')\n",
        "    data_x.to_csv(company + company_code + '.csv', mode='w', encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 종목 리스트 파일 열기\n",
        "# 회사명을 종목코드로 변환\n",
        "def convert_to_code(company,startpage, maxpage):\n",
        "    file_path='/content/drive/MyDrive/company_list.txt'\n",
        "    data = pd.read_csv(file_path, dtype=str, sep='\\t') # 종목코드 추출\n",
        "    company_name = data['회사명']\n",
        "    keys = [i for i in company_name] #데이터프레임에서 리스트로 바꾸기\n",
        "    company_code = data['종목코드']\n",
        "\n",
        "    values = [j for j in company_code]\n",
        "    dict_result = dict(zip(keys, values)) # 딕셔너리 형태로 회사이름과 종목코드 묶기\n",
        "    pattern = '[a-zA-Z가-힣]+'\n",
        "\n",
        "    if bool(re.match(pattern, company)) == True: # Input에 이름으로 넣었을 때\n",
        "        company_code = dict_result.get(str(company))\n",
        "        crawler(company,company_code,startpage ,maxpage)\n",
        "\n",
        "    else: # Input에 종목코드로 넣었을 때\n",
        "        company_code = str(company)\n",
        "        crawler(company,company_code,startpage, maxpage)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    info_main = input(\"=\"*50+\"\\n\"+\"실시간 뉴스기사 다운받기.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
        "    company = input(\"종목 이름이나 코드 입력: \")\n",
        "    startpage = input(\"뉴스 시작 페이지 입력: \")\n",
        "    maxpage = input(\"뉴스 마지막 페이지 입력: \")\n",
        "    convert_to_code(company,startpage, maxpage)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZK7-cymGWFt"
      },
      "source": [
        "# 코드 참조, company_list 다운\n",
        "# https://blog.naver.com/hengju2/222132075354"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiV-4XpQ7Zxb",
        "outputId": "5ba12a9f-1803-4116-f1e3-f06d57f2c5ad"
      },
      "source": [
        "main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "실시간 뉴스기사 다운받기.\n",
            " 시작하시려면 Enter를 눌러주세요.\n",
            "==================================================LG전자\n",
            "종목 이름이나 코드 입력: LG전자\n",
            "뉴스 시작 페이지 입력: 0\n",
            "뉴스 마지막 페이지 입력: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "다운 받고 있습니다------\n",
            "다운 받고 있습니다------\n",
            "다운 받고 있습니다------\n",
            "다운 받고 있습니다------\n",
            "다운 받고 있습니다------\n",
            "다운 받고 있습니다------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
