{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C05_KM_SentimentDict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1G8y9KmSz1CGzmjfrkzJ1-5HNpAAFuYJU",
      "authorship_tag": "ABX9TyOLq4gJDAFdkQciCjVHALUF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyoungmiKwon/Bigdata_Training_at_ITwill/blob/main/Team_Project/C05_KM_SentimentDict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DuftRpn9jyH"
      },
      "source": [
        "# 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8mecskr8OH1",
        "outputId": "08ac3c18-ada8-4649-f43b-abe2dfcefad1"
      },
      "source": [
        "#Konlpy 설치\n",
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 2s (140 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "openjdk-8-jdk is already the newest version (8u292-b10-0ubuntu1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1) (3.7.4.3)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkPWlOp78H9O",
        "outputId": "9a28827d-5139-484b-9c3a-e450aaf7174f"
      },
      "source": [
        "#Mecab 설치\n",
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr-NMjpNHmkM",
        "outputId": "c70e0c11-327e-4d62-86bb-0df95a52905b"
      },
      "source": [
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mecab-ko is already installed\n",
            "mecab-ko-dic is already installed\n",
            "mecab-python is already installed\n",
            "Done.\n",
            "Processing /tmp/mecab-python-0.996\n",
            "Building wheels for collected packages: mecab-python\n",
            "  Building wheel for mecab-python (setup.py): started\n",
            "  Building wheel for mecab-python (setup.py): finished with status 'done'\n",
            "  Created wheel for mecab-python: filename=mecab_python-0.996_ko_0.9.2-cp37-cp37m-linux_x86_64.whl size=141812 sha256=24a0c8cd1b0456a166b87446f9a4764d55004ebd427072eeda64e37e13d09b5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/75/a6/e9e73a1dbd73579383644942ef18a6d17ad728a3052a1147fb\n",
            "Successfully built mecab-python\n",
            "Installing collected packages: mecab-python\n",
            "  Found existing installation: mecab-python 0.996-ko-0.9.2\n",
            "    Uninstalling mecab-python-0.996-ko-0.9.2:\n",
            "      Successfully uninstalled mecab-python-0.996-ko-0.9.2\n",
            "Successfully installed mecab-python-0.996-ko-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzmmC_MHHrlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758de291-000e-4e7a-905c-cb5981d11b66"
      },
      "source": [
        "!git clone https://github.com/entelecheia/eKoNLpy.git\n",
        "!pip install /content/eKoNLpy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'eKoNLpy' already exists and is not an empty directory.\n",
            "Processing ./eKoNLpy\n",
            "Requirement already satisfied: konlpy>=0.4.4 in /usr/local/lib/python3.7/dist-packages (from eKoNLPy==0.5.30) (0.5.2)\n",
            "Requirement already satisfied: nltk>=2.0 in /usr/local/lib/python3.7/dist-packages (from eKoNLPy==0.5.30) (3.2.5)\n",
            "Requirement already satisfied: gensim>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from eKoNLPy==0.5.30) (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from eKoNLPy==0.5.30) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from eKoNLPy==0.5.30) (1.19.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy>=0.4.4->eKoNLPy==0.5.30) (4.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy>=0.4.4->eKoNLPy==0.5.30) (1.2.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy>=0.4.4->eKoNLPy==0.5.30) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy>=0.4.4->eKoNLPy==0.5.30) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy>=0.4.4->eKoNLPy==0.5.30) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=2.0->eKoNLPy==0.5.30) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.1.0->eKoNLPy==0.5.30) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy>=0.4.4->eKoNLPy==0.5.30) (3.1.0)\n",
            "Building wheels for collected packages: eKoNLPy\n",
            "  Building wheel for eKoNLPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eKoNLPy: filename=eKoNLPy-0.5.30-cp37-none-any.whl size=14437140 sha256=3f0e2e0241f58e6f4f4f33e7a0946d88c461952fb38e679ece44d74a5a9524af\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lu9kr_b2/wheels/42/07/b6/a197e00efdc4534f4c1d281bd028c47764df1a63a8b311aa3d\n",
            "Successfully built eKoNLPy\n",
            "Installing collected packages: eKoNLPy\n",
            "  Found existing installation: eKoNLPy 0.5.30\n",
            "    Uninstalling eKoNLPy-0.5.30:\n",
            "      Successfully uninstalled eKoNLPy-0.5.30\n",
            "Successfully installed eKoNLPy-0.5.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvKKZD8B-bA6"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNK1zS1O-eeR"
      },
      "source": [
        "from ekonlpy.tag import Mecab\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx_HYviX-qn-"
      },
      "source": [
        "# Basic Sentiment Dictionary 생성\n",
        "    \n",
        "    Dictionary class for\n",
        "    Korean Sentiment Analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbe0_rkL_kLx"
      },
      "source": [
        "import os\n",
        "from konlpy.tag import Kkma\n",
        "from ekonlpy.sentiment.base import LEXICON_PATH, BaseDict\n",
        "from ekonlpy.sentiment.utils import KTokenizer\n",
        "from ekonlpy.tag import Mecab\n",
        "\n",
        "class KSAA(BaseDict):\n",
        "    '''\n",
        "    Dictionary class for\n",
        "    Korean Sentiment Analysis.\n",
        "    '''\n",
        "\n",
        "    def init_tokenizer(self, kind=None):\n",
        "        self._tokenizer = KTokenizer(self._poldict)\n",
        "\n",
        "    def init_dict(self, kind=None, intensity_cutoff=None):\n",
        "        path = os.path.join('/content/drive/MyDrive/Colab Notebooks/Project/polar_dictionary.csv')\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word = line.split(',')\n",
        "                w = word[0]\n",
        "                if w == 'ngram':\n",
        "                    continue\n",
        "                \n",
        "                p = float(word[6].strip())\n",
        "                n = float(word[3].strip())\n",
        "                s = p - n\n",
        "                if len(w) > 1:\n",
        "                    if s > 0:\n",
        "                        self._posdict[w] = 1\n",
        "                        self._poldict[w] = s\n",
        "                    elif s < 0:\n",
        "                        self._negdict[w] = -1\n",
        "                        self._poldict[w] = s\n",
        "\n",
        "\n",
        "class KOSAC(object):\n",
        "    def __init__(self):\n",
        "        self._loaddic()\n",
        "        self._tagger = Mecab()\n",
        "        self._ngram = 3\n",
        "        self._delimiter = ';'\n",
        "        self._skiptags = ['SF', 'SP', 'SS', 'SE', 'SO', 'SW', 'UN', 'UV', 'UE', 'OL', 'OH', 'ON']\n",
        "\n",
        "    def _loaddic(self):\n",
        "        self._polarity = self._loadfile('/content/drive/MyDrive/Colab Notebooks/Project/polar_dictionary.csv')\n",
        "        self._expressive = self._loadfile(os.path.join(LEXICON_PATH, 'kosac', 'expressive-type.csv'))\n",
        "        self._intensity = self._loadfile(os.path.join(LEXICON_PATH, 'kosac', 'intensity.csv'))\n",
        "\n",
        "    def _loadfile(self, file_path, delimiter=','):\n",
        "        vocab = {}\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path) as f:\n",
        "                for lno, line in enumerate(f):\n",
        "                    # skip header\n",
        "                    if lno == 0:\n",
        "                        headers = line.strip().split(delimiter)\n",
        "                    else:\n",
        "                        if len(line) > 0:\n",
        "                            row = line.strip().split(delimiter)\n",
        "                            ngram = row[0]\n",
        "                            # ngram_split = tuple(ngram.split(';'))\n",
        "                            data = {}\n",
        "                            for i, header in enumerate(headers):\n",
        "                                if i > 0:\n",
        "                                    data[header] = row[i]\n",
        "                            vocab[ngram] = data\n",
        "        return vocab\n",
        "\n",
        "    def morpheme(self, dataset):\n",
        "        return self.align_morpheme(self._tagger.pos(dataset))\n",
        "\n",
        "    def align_morpheme(self, morpheme):\n",
        "        return ['{}/{}'.format(w, t) for w, t in morpheme]\n",
        "\n",
        "    def percentage(self, obj):\n",
        "        return {k: v / sum(obj.values()) for k, v in obj.items()}\n",
        "\n",
        "    def calc(self, keypairs, source, target, func):\n",
        "        for keypair in keypairs:\n",
        "            sourcekey = keypair[0]\n",
        "            targetkey = keypair[1]\n",
        "            if sourcekey in source:\n",
        "                sourcedata = source[sourcekey]\n",
        "                sourcedata = float(sourcedata)\n",
        "                target[targetkey] = func(sourcedata, target[targetkey])\n",
        "        return target\n",
        "\n",
        "    def match(self, data, pairdata, keypairs):\n",
        "        ret = {k[1]: 0 for k in keypairs}\n",
        "        for m in data:\n",
        "            if m in pairdata:\n",
        "                currentdata = pairdata[m]\n",
        "                ret = self.calc(keypairs, currentdata, ret,\n",
        "                                lambda s, t: t + s)\n",
        "        return self.percentage(ret)\n",
        "\n",
        "    def polarity(self, data):\n",
        "        return self.match(data,\n",
        "                          self._polarity,\n",
        "                          [['COMP', 'com'],\n",
        "                           ['POS', 'pos'],\n",
        "                           ['NEG', 'neg'],\n",
        "                           ['NEUT', 'neut'],\n",
        "                           ['None', 'none']])\n",
        "\n",
        "    def intensity(self, data):\n",
        "        return self.match(data,\n",
        "                          self._intensity,\n",
        "                          [['High', 'high'],\n",
        "                           ['Low', 'low'],\n",
        "                           ['Medium', 'medium'],\n",
        "                           ['None', 'none']])\n",
        "\n",
        "    def expressive(self, data):\n",
        "        return self.match(data,\n",
        "                          self._expressive,\n",
        "                          [['dir-action', 'dir-action'],\n",
        "                           ['dir-explicit', 'dir-explicit'],\n",
        "                           ['dir-speech', 'dir-speech'],\n",
        "                           ['indirect', 'indirect'],\n",
        "                           ['writing-device', 'writing-device']])\n",
        "\n",
        "    def analyze(self, dataset):\n",
        "        dataset = self.parse(dataset)\n",
        "        ret = {}\n",
        "        for analysis in ['polarity', 'intensity', 'expressive']:\n",
        "            func = getattr(self, analysis)\n",
        "            ret[analysis] = func(dataset)\n",
        "        return ret\n",
        "\n",
        "    def parse(self, dataset):\n",
        "        tokens = []\n",
        "        if type(dataset) == list:\n",
        "            for t in dataset:\n",
        "                tokens += self.morpheme(t)\n",
        "        elif type(dataset) == str:\n",
        "            tokens = self.morpheme(dataset)\n",
        "        else:\n",
        "            raise ValueError('The dataset has to be string or list of string type.')\n",
        "\n",
        "        return self.ngramize(tokens)\n",
        "\n",
        "    def ngramize(self, tokens):\n",
        "        ngram_tokens = []\n",
        "        tokens = [w for w in tokens if w.split('/')[1] not in self._skiptags]\n",
        "        for pos in range(len(tokens)):\n",
        "            for gram in range(1, self._ngram + 1):\n",
        "                token = self.get_ngram(tokens, pos, gram)\n",
        "                if token:\n",
        "                    ngram_tokens.append(token)\n",
        "        return ngram_tokens\n",
        "\n",
        "    def get_ngram(self, tokens, pos, gram):\n",
        "        if pos < 0:\n",
        "            return None\n",
        "        if pos + gram > len(tokens):\n",
        "            return None\n",
        "        token = tokens[pos]\n",
        "        for i in range(1, gram):\n",
        "            token += self._delimiter + tokens[pos + i]\n",
        "        return token"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayop21YGAiFV"
      },
      "source": [
        "## 감성 극성계산 위한 함수들 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3GggKjH-yOv"
      },
      "source": [
        "import abc\n",
        "from ekonlpy.utils import installpath\n",
        "\n",
        "LEXICON_PATH = '%s/data/lexicon' % installpath\n",
        "\n",
        "\n",
        "class BaseDict(object):\n",
        " \n",
        "\n",
        "    __metaclass__ = abc.ABCMeta\n",
        "\n",
        "    TAG_POL = 'Polarity'\n",
        "    TAG_SUB = 'Subjectivity'\n",
        "    TAG_POS = 'Positive'\n",
        "    TAG_NEG = 'Negative'\n",
        "\n",
        "    EPSILON = 1e-6\n",
        "\n",
        "    def __init__(self, tokenizer=None, kind=None, intensity_cutoff=None):\n",
        "        self._posdict = {}\n",
        "        self._negdict = {}\n",
        "        self._poldict = {}\n",
        "        self._intensity_cutoff = intensity_cutoff\n",
        "        self.init_dict(kind, intensity_cutoff)\n",
        "        if tokenizer is None:\n",
        "            self.init_tokenizer(kind)\n",
        "        else:\n",
        "            self._tokenizer = tokenizer\n",
        "\n",
        "        assert len(self._posdict) > 0 and len(self._negdict) > 0\n",
        "\n",
        "    def tokenize(self, text):\n",
        "\n",
        "        return self._tokenizer.tokenize(text)\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def init_tokenizer(self, kind):\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def init_dict(self, kind, intensity_cutoff):\n",
        "        pass\n",
        "\n",
        "    def _get_score(self, term, by_count=True):\n",
        "\n",
        "        if by_count:\n",
        "            if term in self._posdict.keys():\n",
        "                return self._posdict[term]\n",
        "            elif term in self._negdict.keys():\n",
        "                return self._negdict[term]\n",
        "            else:\n",
        "                return 0\n",
        "        else:\n",
        "            if term in self._poldict.keys():\n",
        "                return self._poldict[term]\n",
        "            else:\n",
        "                return 0\n",
        "\n",
        "    def get_score(self, terms, by_count=True):\n",
        "\n",
        "        assert isinstance(terms, list) or isinstance(terms, tuple)\n",
        "        score_li = [self._get_score(t, by_count) for t in terms]\n",
        "        pos_score_li = [s for s in score_li if s > 0]\n",
        "        neg_score_li = [s for s in score_li if s < 0]\n",
        "\n",
        "        s_pos = sum(pos_score_li)\n",
        "        s_neg = sum(neg_score_li)\n",
        "\n",
        "        s_pol = (s_pos + s_neg) * 1.0 / (((s_pos - s_neg) if by_count else len(score_li)) + self.EPSILON)\n",
        "        s_sub = (len(pos_score_li) + len(neg_score_li)) * 1.0 / (len(score_li) + self.EPSILON)\n",
        "\n",
        "        return {self.TAG_POS: s_pos,\n",
        "                self.TAG_NEG: s_neg,\n",
        "                self.TAG_POL: s_pol,\n",
        "                self.TAG_SUB: s_sub}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_BU6__w_ChO"
      },
      "source": [
        "# Tokenkizer 생성 using in Sentiment dictionary\n",
        "\n",
        "    An abstract class for tokenize text.\n",
        "\n",
        "    The default tokenizer for KSA sub class.\n",
        "    The output of the tokenizer is tagged by Kkma.   \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1VdPVxP-2z1"
      },
      "source": [
        "import abc\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "from ekonlpy.tag import Mecab\n",
        "from ekonlpy.sentiment.base import LEXICON_PATH\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "\n",
        "class BaseTokenizer(object):\n",
        "    '''\n",
        "    An abstract class for tokenize text.\n",
        "    '''\n",
        "\n",
        "    __metaclass__ = abc.ABCMeta\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def tokenize(self, text):\n",
        "        '''Return tokenized temrs.\n",
        "        \n",
        "        :type text: str\n",
        "        \n",
        "        :returns: list \n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    # @abc.abstractmethod\n",
        "    # def ngramize(self, tokens):\n",
        "    #     '''Return n-gramized temrs.\n",
        "    #\n",
        "    #     :type tokens: list of tokens\n",
        "    #\n",
        "    #     :returns: list\n",
        "    #     '''\n",
        "    #     pass\n",
        "\n",
        "\n",
        "class KTokenizer(BaseTokenizer):\n",
        "    '''\n",
        "    The default tokenizer for KSA sub class.\n",
        "    The output of the tokenizer is tagged by Kkma.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, vocab=None):\n",
        "        self._tagger = Kkma()\n",
        "        self._vocab = vocab\n",
        "        self._min_ngram = 1\n",
        "        self._ngram = 3\n",
        "        self._delimiter = ';'\n",
        "        self._skiptags = ['SF', 'SP', 'SS', 'SE', 'SO', 'SW', 'UN', 'UV', 'UE', 'OL', 'OH', 'ON']\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = []\n",
        "        if type(text) == list:\n",
        "            for t in text:\n",
        "                tokens += self.morpheme(t)\n",
        "        elif type(text) == str:\n",
        "            tokens = self.morpheme(text)\n",
        "        else:\n",
        "            raise ValueError('The dataset has to be string or list of string type.')\n",
        "\n",
        "        return self.ngramize(tokens)\n",
        "\n",
        "    def ngramize(self, tokens):\n",
        "        ngram_tokens = []\n",
        "        tokens = [w for w in tokens if w.split('/')[1] not in self._skiptags]\n",
        "        for pos in range(len(tokens)):\n",
        "            for gram in range(1, self._ngram + 1):\n",
        "                token = self.get_ngram(tokens, pos, gram)\n",
        "                if token:\n",
        "                    if self._vocab is None:\n",
        "                        ngram_tokens.append(token)\n",
        "                    else:\n",
        "                        if token in self._vocab:\n",
        "                            ngram_tokens.append(token)\n",
        "        return ngram_tokens\n",
        "\n",
        "    def get_ngram(self, tokens, pos, gram):\n",
        "        if pos < 0:\n",
        "            return None\n",
        "        if pos + gram > len(tokens):\n",
        "            return None\n",
        "        token = tokens[pos]\n",
        "        for i in range(1, gram):\n",
        "            token += self._delimiter + tokens[pos + i]\n",
        "        return token\n",
        "\n",
        "    def morpheme(self, dataset):\n",
        "        return self.align_morpheme(self._tagger.pos(dataset))\n",
        "\n",
        "    def align_morpheme(self, morpheme):\n",
        "        return ['{}/{}'.format(w, t) for w, t in morpheme]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MhHcXl-DidQ"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "gRQjki7RDlpt",
        "outputId": "6757946e-f0e6-4a14-f050-1c63f4e9959d"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Project/news_dt.xlsx'\n",
        "data = pd.read_excel(file_path,sheet_name='Sheet1',header=0)\n",
        "data[:2]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Code</th>\n",
              "      <th>Date</th>\n",
              "      <th>Journal</th>\n",
              "      <th>Title</th>\n",
              "      <th>Text</th>\n",
              "      <th>URL</th>\n",
              "      <th>Sample Y/N</th>\n",
              "      <th>In Charge</th>\n",
              "      <th>S</th>\n",
              "      <th>seperate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14</td>\n",
              "      <td>5930</td>\n",
              "      <td>2021.04.14 17:40</td>\n",
              "      <td>한국경제</td>\n",
              "      <td>삼성 '13조 상속세'…연부연납 활용, 올해 낼 2兆 중 1.2兆 대출...</td>\n",
              "      <td>삼성 조 상속세 이재용 부회장 등  재원 마련은보유주식 담보대출  적용나머지는 부동...</td>\n",
              "      <td>https://finance.naver.com/item/news_read.nhn?a...</td>\n",
              "      <td>A</td>\n",
              "      <td>JY</td>\n",
              "      <td>0</td>\n",
              "      <td>['상속세', '이재용', '재원', '마련', '보유', '적용', '나머지', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>5930</td>\n",
              "      <td>2021.04.14 16:49</td>\n",
              "      <td>이데일리</td>\n",
              "      <td>삼성, 車반도체 생산 가능성은…\"수익성·기존 고객사 문제로 어려워\"</td>\n",
              "      <td>인텔 반도체 회의 직후 반도체 생산 생산 전망삼성 뛰어들기 어려울 것수익성 낮고 제...</td>\n",
              "      <td>https://finance.naver.com/item/news_read.nhn?a...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SW</td>\n",
              "      <td>-1</td>\n",
              "      <td>['회의', '직후', '생산', '생산', '전망', '수익성', '제조', '인...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   No  Code  ...  S                                           seperate\n",
              "0  14  5930  ...  0  ['상속세', '이재용', '재원', '마련', '보유', '적용', '나머지', ...\n",
              "1  15  5930  ... -1  ['회의', '직후', '생산', '생산', '전망', '수익성', '제조', '인...\n",
              "\n",
              "[2 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N_Zny32D-9O",
        "outputId": "9cf9363f-06e9-430e-cbe1-5bdee7cf32a1"
      },
      "source": [
        "data.groupby(['S'])['No'].count()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "S\n",
              "-1    308\n",
              " 0    371\n",
              " 1    675\n",
              "Name: No, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYXos79JGt1I"
      },
      "source": [
        "# 셋트 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF-kHexAGyt2"
      },
      "source": [
        "# 부정,중립,긍정으로 할 경우\n",
        "X = data['Text']\n",
        "y = data['S']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV9Qx5nNG3lu"
      },
      "source": [
        "# 긍정, 긍정이 아닌 경우 : 이진분류로 치환시\n",
        "y_1 = data['S'].replace(0,-1)\n",
        "y = pd.DataFrame(y_1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UswKH07sG6K1"
      },
      "source": [
        "X_tr, X_ts, y_tr, y_ts = train_test_split(X,y,             \n",
        "                                          test_size = 0.2,\n",
        "                                          stratify = y)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "hhrUwFbUG84f",
        "outputId": "5ff5d619-5306-40db-ba01-7f331191d64b"
      },
      "source": [
        "std_data = pd.concat([X_tr,y_tr],axis=1, ignore_index=False)\n",
        "std_data[10:15]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>중국군에 들어간  생산칩에대량살상무기 안보논리 대응중국 반도체 설계 블랙리스트 등...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>서 롤러블폰 공개해 주목올 상반기 출시로부터 플렉시블  공급가격적 측면 고려예상 ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>804</th>\n",
              "      <td>삼성전자가 지난해 미중 무역갈등과 신종 코로나바이러스 감염증코로나 등 불확실성 속에...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>삼성 반도체  투자 순위년 위년 위로 올라서  년째 위미래 먹거리 확보 위한  투자...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>922</th>\n",
              "      <td>삼성전자는 지난해 연간 영업이익이 조억원으로 전년 대비  증가한 것으로 잠정 집계됐...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Text  S\n",
              "34    중국군에 들어간  생산칩에대량살상무기 안보논리 대응중국 반도체 설계 블랙리스트 등... -1\n",
              "605   서 롤러블폰 공개해 주목올 상반기 출시로부터 플렉시블  공급가격적 측면 고려예상 ... -1\n",
              "804  삼성전자가 지난해 미중 무역갈등과 신종 코로나바이러스 감염증코로나 등 불확실성 속에...  1\n",
              "209  삼성 반도체  투자 순위년 위년 위로 올라서  년째 위미래 먹거리 확보 위한  투자... -1\n",
              "922  삼성전자는 지난해 연간 영업이익이 조억원으로 전년 대비  증가한 것으로 잠정 집계됐...  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNcuS2fqND5c",
        "outputId": "cd39d3b3-45cc-4459-ad71-aebbcf88f342"
      },
      "source": [
        "len(data['Text'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1354"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NH68_XKI3u2"
      },
      "source": [
        "ksaa = KSAA()\n",
        "\n",
        "pola_std_text = []\n",
        "for i in range(len(std_data['Text'])): # 기사 하나 씩\n",
        "\n",
        "    polar = 0\n",
        "    for k in std_data['Text']:         # 하나의 기사에서 한 문장   \n",
        "        tokens = ksaa.tokenize(k)\n",
        "\n",
        "        token_lst = []\n",
        "        for a in tokens:                    #> ['가/VV', '되/XSV', '확산/NNG', '따르/VV', '거리/NNG', '기/NNG', '정착/NNG', '시장/NNG', '이/JKS;불/VV', '불/VV', '등/NNB', '등/NNB;을/JKO'......]\n",
        "            one_dot = a.split('/')          #> ['확산', 'NNG']\n",
        "            if len(one_dot[0]) != int(1):   #>  '확산'\n",
        "                join_dot= '/'.join(one_dot) #> 확산/NNG\n",
        "                token_lst.append(join_dot)\n",
        "        \n",
        "        if len(token_lst) < int(2) :\n",
        "            token_lst=[]\n",
        "        \n",
        "        # print(token_lst)\n",
        "        score = ksaa.get_score(token_lst)   #> {'Positive': 55, 'Negative': -18, 'Polarity': 0.506849308125352, 'Subjectivity': 0.9999999863013701}\n",
        "        polar += score['Polarity']\n",
        "        sen_num = len(k) #> 문장 수\n",
        "    \n",
        "    pola_std_text.append(polar/sen_num)\n",
        "\n",
        "std_data['polarity_std'] = pola_std_text\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}