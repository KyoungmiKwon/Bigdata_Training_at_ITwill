{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled20.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOkBC9ukvH7qBcoTirLk5rh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyoungmiKwon/Bigdata_Training_at_ITwill/blob/main/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD2o40sEGwbY"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0ez1k8xGyEd"
      },
      "source": [
        "code = {'삼성전자':'005930', 'LG전자':'066570', 'JYP_Ent':'035900', 'YG_ENt':'122870'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0bcBuW9G1Zt"
      },
      "source": [
        "#> 함수(종목코드, 시작페이지, 종료페이지, 저장할 파일이름)\n",
        "def crawling_news(code_num, start_page_num=1, end_page_num=2, file_name='test.csv'):\n",
        "\n",
        "    # title, 본문url selector + 관련 뉴스의 title, 본문url\n",
        "    title_selector = 'body > div > table.type5 > tbody > tr > td.title > a, tr.relation_lst > td > table > tbody > tr > td.title > a'\n",
        "    # time selector + 관련 뉴스의 time\n",
        "    time_selector = 'body > div > table.type5 > tbody > tr > td.date, tr.relation_lst > td > table > tbody > tr > td.date'\n",
        "    # 본문 내용 selector\n",
        "    content_selector = 'div#news_read'\n",
        "\n",
        "    # 데이터 저장\n",
        "    data = {}\n",
        "    data_titles = []\n",
        "    data_address = []\n",
        "    data_times = []\n",
        "    data_contents = []\n",
        "    break_flag = False      #> 마지막에 더이상 뉴스가 없을때 마지막 뉴스가 중복되어서 나옴, 그럴 때 종료를 하기 위한 flag\n",
        "\n",
        "    # start 부터 end 까지 반복문\n",
        "    for page_num in range(start_page_num,end_page_num+1):\n",
        "        url = f'https://finance.naver.com/item/news_news.nhn?code={code_num}&page={page_num}'\n",
        "        res = requests.get(url)\n",
        "        html = res.text\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        #soup = BeautifulSoup(html, 'html.parser')\n",
        "        \n",
        "        titles = soup.select(title_selector)\n",
        "        times = soup.select(time_selector)\n",
        "\n",
        "        for title in titles:\n",
        "            # title, address 추가\n",
        "            if len(data_address) != 0:  #> 처음 시작에는 종료가 발생하지 않도록\n",
        "                if data_titles[len(data_titles)-1] == title.text: #> 이전 뉴스의 title과 같은 title 이면\n",
        "                    break_flag = True   # 종료 flag를 바꿔주고 for문 break\n",
        "                    break\n",
        "            data_titles.append(title.text) \n",
        "            data_address.append('https://finance.naver.com/'+title.get('href'))\n",
        "            \n",
        "            # contents 추가\n",
        "            url = 'https://finance.naver.com/'+title.get('href') #> 저장한 본문 url 불러오기\n",
        "            res = requests.get(url)\n",
        "            html = res.text\n",
        "            soup = BeautifulSoup(html, 'lxml')\n",
        "            #soup = BeautifulSoup(html, 'html.parser')\n",
        "            contents = soup.select(content_selector)\n",
        "            for content in contents:\n",
        "                data_contents.append(content.text.split(sep='@')[0])\n",
        "        \n",
        "        if break_flag == True:  #> 종료 flag가 on이면 종료 == 마지막뉴스까지 긁었다.\n",
        "            print('마지막 뉴스 => 종료')\n",
        "            break\n",
        "\n",
        "        for time in times:\n",
        "            data_times.append(time.text)\n",
        "\n",
        "        data['title'] = data_titles\n",
        "        data['time'] = data_times\n",
        "        data['address'] = data_address\n",
        "        data['content'] = data_contents\n",
        "\n",
        "    #> dataframe 형식으로 저장\n",
        "    result = pd.DataFrame(data)\n",
        "    \n",
        "    # 시간순으로 sort 하고 덮어쓰기\n",
        "    result.sort_values(by=['time'], axis=0, ascending=False, inplace=True)\n",
        "\n",
        "    # 중복된 기사들 있어서 추가, title이 중복되면 가장 최신(시간순sort)의 기사 제외 삭제, 수정사항 덮어쓰기\n",
        "    result.drop_duplicates(['title'], keep='first', inplace=True)\n",
        "\n",
        "    # CSV 파일로 저장\n",
        "    result.to_csv(file_name, sep=',', na_rep='NaN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbnJRS2cG3YY"
      },
      "source": [
        "crawling_news(code['삼성전자'], 16, 20, 'SAMSUNG.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PGpCel1G9wo"
      },
      "source": [
        "test = pd.read_csv('/content/SAMSUNG.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo8jBwJ-HJg3"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLDMvjROHLBq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}